releases:

################################################################################
## TensorFlow-Serving ##########################################################
################################################################################

#
# References:
#   - https://github.com/vanvalenlab/kiosk-console/blob/master/conf/charts/tf-serving/values.yaml
#
- name: tf-serving
  namespace: deepcell
  labels:
    chart: tf-serving
    component: deepcell
    namespace: deepcell
    vendor: vanvalenlab
    default: true
  chart: '{{ env "CHARTS_PATH" | default "/conf/charts" }}/tf-serving'
  version: 0.4.0
  wait: true
  timeout: 300
  atomic: true
  cleanupOnFail: true
  values:
    - replicas: 0

      image:
        repository: vanvalenlab/kiosk-tf-serving
        tag: 0.4.0
      
      configWriter:
        mountedVolume:
          name: configdir
          path: /config

        image:
          repository: vanvalenlab/kiosk-tf-serving-config-writer
          tag: 0.4.0
          pullPolicy: IfNotPresent

      resources:
        requests:
          cpu: 1
          memory: 3.5Gi
        limits:
          nvidia.com/gpu: 1
          # cpu: 2
          # memory: 8Gi

      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      - key: prediction_gpu
        operator: Exists
        effect: NoSchedule

      service:
        type: ClusterIP
        httpIngressEnabled: true
        grpcIngressEnabled: true
        httpsIngressEnabled: false

        annotations:
          prometheus.io/path: /monitoring/prometheus/metrics
          prometheus.io/port: "8501"
          prometheus.io/scrape: "true"

      hpa:
        enabled: true
        minReplicas: 1
        maxReplicas: {{ int (env "GPU_NODE_MAX_SIZE" | default 1) }}
        metrics:
        - type: Object
          object:
            metricName: tf_serving_gpu_usage
            target:
              apiVersion: v1
              kind: Namespace
              name: tf_serving_gpu_usage
            targetValue: 70

      annotations:
        prometheus.io/path: /monitoring/prometheus/metrics
        prometheus.io/port: "8501"
        prometheus.io/scrape: "true"

      nodeSelector:
{{ if eq (env "CLOUD_PROVIDER" | default "aws") "aws" }}
        beta.kubernetes.io/instance-type: '{{ env "AWS_GPU_MACHINE_TYPE" | default "p2.xlarge" }}'
{{ else }}
        cloud.google.com/gke-accelerator: '{{ env "GCP_PREDICTION_GPU_TYPE" | default "nvidia-tesla-t4" }}'
        cloud.google.com/gke-preemptible: "true"
{{ end }}

      env:
        PORT: 8500
        REST_API_PORT: 8501
        REST_API_TIMEOUT: 30000
        MODEL_CONFIG_FILE: /config/models.conf
        BATCHING_CONFIG_FILE: /config/batching_config.txt
        ENABLE_BATCHING: "true"
        MAX_BATCH_SIZE: 128
        BATCH_TIMEOUT_MICROS: 0
        MAX_ENQUEUED_BATCHES: 512
        GRPC_CHANNEL_ARGS: ""
        MODEL_PREFIX: models
        TF_CPP_MIN_LOG_LEVEL: 0
        TF_SESSION_PARALLELISM: 0
        MONITORING_CONFIG_FILE: /config/monitoring_config.txt
        PROMETHEUS_MONITORING_ENABLED: "true"
        PROMETHEUS_MONITORING_PATH: /monitoring/prometheus/metrics

      secrets:
        AWS_ACCESS_KEY_ID: '{{ env "AWS_ACCESS_KEY_ID" | default "NA" }}'
        AWS_SECRET_ACCESS_KEY: '{{ env "AWS_SECRET_ACCESS_KEY" | default "NA" }}'
        # Uncomment to override the default models.
{{ if eq (env "CLOUD_PROVIDER" | default "aws") "aws" }}
        # STORAGE_BUCKET: 's3://{{ env "AWS_S3_BUCKET" | default "NA" }}'
{{ else }}
        # STORAGE_BUCKET: 'gs://{{ env "CLOUDSDK_BUCKET" | default "NA" }}'
{{ end }}
