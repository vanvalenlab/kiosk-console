helmDefaults:
  args:
    - "--wait"
    - "--timeout=600"
    - "--force"
    - "--reset-values"

releases:

################################################################################
## ElasticSearch ###############################################################
################################################################################

#
# References:
#   - [web address of Helm chart's YAML file]
#
- name: "elasticsearch"
  namespace: "elk"
  labels:
    chart: "elasticsearch"
    component: "elasticsearch"
    namespace: "elk"
    vendor: "elastic.co"
    default: "true"
  chart: 'stable/elasticsearch'
  version: "1.19.1"
  values:
    - appVersion: "6.6.0"
      serviceAccounts:
        client:
          create: true
          name:
        master:
          create: true
          name:
        data:
          create: true
          name:
      podSecurityPolicy:
        enabled: false
        annotations: {}
          ## Specify pod annotations
          ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor
          ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp
          ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl
          ##
          # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
          # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'
          # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'
      image:
        repository: "docker.elastic.co/elasticsearch/elasticsearch-oss"
        tag: "6.6.0"
        pullPolicy: "IfNotPresent"
      initImage:
        repository: "busybox"
        tag: "latest"
        pullPolicy: "Always"
      cluster:
        name: "elasticsearch"
        xpackEnable: false
        config: {}
        additionalJavaOpts: ""
        bootstrapShellCommand: ""
        env:
          MINIMUM_MASTER_NODES: "2"
      client:
        name: client
        replicas: 2
        serviceType: ClusterIP
        loadBalancerIP: {}
        loadBalancerSourceRanges: {}
      ## (dict) If specified, apply these annotations to the client service
      #  serviceAnnotations:
      #    example: client-svc-foo
        heapSize: "512m"
        # additionalJavaOpts: "-XX:MaxRAM=512m"
        antiAffinity: "soft"
        nodeAffinity: {}
        nodeSelector: {}
        tolerations: []
        initResources: {}
          # limits:
          #   cpu: "25m"
          #   # memory: "128Mi"
          # requests:
          #   cpu: "25m"
          #   memory: "128Mi"
        resources:
          limits:
            cpu: "1"
            # memory: "1024Mi"
          requests:
            cpu: "25m"
            memory: "768Mi"
        priorityClassName: ""
        ## (dict) If specified, apply these annotations to each client Pod
        # podAnnotations:
        #   example: client-foo
        podDisruptionBudget:
          enabled: false
          minAvailable: 1
          # maxUnavailable: 1
        ingress:
          enabled: false
          annotations: {}
            # kubernetes.io/ingress.class: nginx
            # kubernetes.io/tls-acme: "true"
          path: /
          hosts:
            - chart-example.local
          tls: []
          #  - secretName: chart-example-tls
          #    hosts:
          #      - chart-example.local
      master:
        name: master
        exposeHttp: false
        replicas: 3
        heapSize: "512m"
        # additionalJavaOpts: "-XX:MaxRAM=512m"
        persistence:
          enabled: true
          accessMode: ReadWriteOnce
          name: data
          size: "4Gi"
          # storageClass: "ssd"
        readinessProbe:
          httpGet:
            path: /_cluster/health?local=true
            port: 9200
          initialDelaySeconds: 30
        antiAffinity: "soft"
        nodeAffinity: {}
        nodeSelector: {}
        tolerations: []
        initResources: {}
          # limits:
          #   cpu: "25m"
          #   # memory: "128Mi"
          # requests:
          #   cpu: "25m"
          #   memory: "128Mi"
        resources:
          limits:
            cpu: "1"
            # memory: "1024Mi"
          requests:
            cpu: "25m"
            memory: "768Mi"
        priorityClassName: ""
        ## (dict) If specified, apply these annotations to each master Pod
        # podAnnotations:
        #   example: master-foo
        podDisruptionBudget:
          enabled: false
          minAvailable: 2  # Same as `cluster.env.MINIMUM_MASTER_NODES`
          # maxUnavailable: 1
        updateStrategy:
          type: OnDelete
      data:
        name: data
        exposeHttp: false
        replicas: 2
        heapSize: "1536m"
        # additionalJavaOpts: "-XX:MaxRAM=1536m"
        persistence:
          enabled: true
          accessMode: ReadWriteOnce
          name: data
          size: "30Gi"
          # storageClass: "ssd"
        readinessProbe:
          httpGet:
            path: /_cluster/health?local=true
            port: 9200
          initialDelaySeconds: 30
        terminationGracePeriodSeconds: 3600
        antiAffinity: "soft"
        nodeAffinity: {}
        nodeSelector:
          elasticsearch_data: "yes"
        tolerations:
        - key: elasticsearch_data
          operator: Exists
          effect: NoSchedule
        initResources: {}
          # limits:
          #   cpu: "25m"
          #   # memory: "128Mi"
          # requests:
          #   cpu: "25m"
          #   memory: "128Mi"
        resources:
          limits:
            ### Resource limits removed because elasticsearch-data-* pods are being scheduled to their own nodes now
            # cpu: "1"
            # memory: "2048Mi"
          requests:
            cpu: "300m"
            # memory: "10Gi"
        priorityClassName: ""
        ## (dict) If specified, apply these annotations to each data Pod
        # podAnnotations:
        #   example: data-foo
        podDisruptionBudget:
          enabled: false
          # minAvailable: 1
          maxUnavailable: 1
        updateStrategy:
          type: OnDelete
        hooks:  # post-start and pre-stop hooks
          drain:  # drain the node before stopping it and re-integrate it into the cluster after start
            enabled: true
      extraInitContainers: |
