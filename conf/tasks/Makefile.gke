# FIXME: GPU AZs and GKE AZ might not be same

export CLOUDSDK_CONFIG ?= /localhost/.config/gcloud/

# https://cloud.google.com/kubernetes-engine/docs/quickstart
# https://cloud.google.com/compute/docs/machine-types
# https://cloud.google.com/compute/pricing
#
# Increase quotas: https://console.cloud.google.com/iam-admin/quotas

## Login to Google Cloud
gke/login:
	@gcloud-login.sh

## Create a new project
gke/create/project:
	@gcloud projects create $(CLOUDSDK_CORE_PROJECT)
#	@gcloud alpha billing accounts projects link $(CLOUDSDK_CORE_PROJECT --account-id=$(BILLING_ACCOUNT_ID)

## Destroy project
gke/destroy/project:
	@gcloud projects delete $(CLOUDSDK_CORE_PROJECT)

## Create a new GKE cluster
gke/create/cluster:
	@echo "Creating GKE cluster in zones: ${REGION_ZONES_WITH_GPUS}..."
	gcloud container clusters create $(CLOUDSDK_CONTAINER_CLUSTER) \
		--service-account=$(GCP_SERVICE_ACCOUNT) \
		--region=$(CLOUDSDK_COMPUTE_REGION) \
		--node-locations=$(REGION_ZONES_WITH_GPUS) \
		--max-nodes=$(NODE_MAX_SIZE) \
		--min-nodes=$(NODE_MIN_SIZE) \
		--machine-type=$(GKE_MACHINE_TYPE) \
		--cluster-version $(KUBERNETES_VERSION) \
		--enable-autoscaling \
		--no-enable-autoupgrade
	@echo "GKE cluster creation complete."
	@echo " "
	@echo " "

## Destroy GKE cluster
gke/destroy/cluster:
	@echo "Destroying cluster..."
	@-gcloud container clusters --region $(CLOUDSDK_COMPUTE_REGION) delete $(CLOUDSDK_CONTAINER_CLUSTER) --quiet
	@echo "Cluster destruction finished."
	@echo " "
	@echo " "

## Set context to use GKE cluster (e.g. with kubectl)
gke/use/cluster:
	@gcloud config set project $(CLOUDSDK_CORE_PROJECT)
	@gcloud container clusters get-credentials $(CLOUDSDK_CONTAINER_CLUSTER)
	@gcloud config set compute/region $(CLOUDSDK_COMPUTE_REGION)

## List all GKE projects
gke/list/projects:
	@gcloud projects list

## List all availability zones
gke/list/zones:
	@gcloud compute zones list

## List all accelerator machine types
gke/list/accelerator-types:
	@gcloud compute accelerator-types list

## List all billing accounts
gke/list/billing-accounts:
	@gcloud alpha billing accounts list

## Node pool creation
gke/create/consumer-node-pool:
	@echo "Creating consumer CPU node pool..."
	@echo "Using the following command: "
	gcloud container node-pools create consumer-cpu \
		--cluster $(CLOUDSDK_CONTAINER_CLUSTER) \
		--service-account=$(GCP_SERVICE_ACCOUNT) \
		--region $(CLOUDSDK_COMPUTE_REGION) \
		--num-nodes 1 \
		--min-nodes 1 \
		--max-nodes ${NODE_MAX_SIZE} \
		--machine-type=${CONSUMER_MACHINE_TYPE} \
		--enable-autoscaling \
		--enable-autorepair \
		--no-enable-autoupgrade \
		--preemptible \
		--node-labels consumer=yes \
		--node-taints consumer=yes:NoSchedule

gke/create/prediction-node-pool:
	@echo "Creating prediction GPU node pool..."
	@echo "Using the following command: "
	gcloud container node-pools create prediction-gpu \
		--cluster $(CLOUDSDK_CONTAINER_CLUSTER) \
		--service-account=$(GCP_SERVICE_ACCOUNT) \
		--accelerator type=$(GCP_PREDICTION_GPU_TYPE),count=$(GPU_PER_NODE) \
		--region $(CLOUDSDK_COMPUTE_REGION) \
		--num-nodes $(GPU_NODE_MIN_SIZE) \
		--min-nodes $(GPU_NODE_MIN_SIZE) \
		--max-nodes $(GPU_NODE_MAX_SIZE) \
		--machine-type=$(GPU_MACHINE_TYPE) \
		--enable-autoscaling \
		--enable-autorepair \
		--no-enable-autoupgrade \
		--preemptible \
		--node-taints prediction_gpu=yes:NoSchedule

gke/create/training-node-pool:
	@echo "Creating training GPU node pool..."
	@echo "Using the following command: "
	gcloud container node-pools create training-gpu \
		--cluster $(CLOUDSDK_CONTAINER_CLUSTER) \
		--accelerator type=$(GCP_TRAINING_GPU_TYPE),count=$(GPU_PER_NODE) \
		--service-account=$(GCP_SERVICE_ACCOUNT) \
		--region $(CLOUDSDK_COMPUTE_REGION) \
		--num-nodes $(GPU_NODE_MIN_SIZE) \
		--min-nodes $(GPU_NODE_MIN_SIZE) \
		--max-nodes $(GPU_NODE_MAX_SIZE) \
		--machine-type=$(GPU_MACHINE_TYPE) \
		--enable-autoscaling \
		--enable-autorepair \
		--no-enable-autoupgrade \
		--node-taints training_gpu=yes:NoSchedule

# https://cloud.google.com/kubernetes-engine/docs/how-to/gpus
## Create GKE GPU node pool
#		--enable-autoupgrade
gke/create/node-pools: \
	gke/create/consumer-node-pool \
	gke/create/prediction-node-pool
	# gke/create/training-node-pool
	@echo "All node pools created."
	@echo " "
	@echo " "

gke/create/elk-node-pools:
	@if [ -n "${ELK_DEPLOYMENT_TOGGLE}" ]; then\
		echo "Creating elasticsearch CPU node pool...";\
		echo "Using the following command: ";\
		gcloud container node-pools create elasticsearch-cpu \
			--cluster $(CLOUDSDK_CONTAINER_CLUSTER) \
			--service-account=$(GCP_SERVICE_ACCOUNT) \
			--region $(CLOUDSDK_COMPUTE_REGION) \
			--num-nodes 2 \
			--min-nodes 1 \
			--max-nodes 2 \
			--machine-type=n1-highmem-2 \
			--enable-autoscaling \
			--enable-autorepair \
			--no-enable-autoupgrade \
			--preemptible \
			--node-labels elasticsearch_data=yes \
			--node-taints elasticsearch_data=yes:NoSchedule; \
		echo "Creating logstash CPU node pool..."; \
		echo "Using the following command: "; \
		gcloud container node-pools create logstash-cpu \
			--cluster $(CLOUDSDK_CONTAINER_CLUSTER) \
			--service-account=$(GCP_SERVICE_ACCOUNT) \
			--region $(CLOUDSDK_COMPUTE_REGION) \
			--num-nodes 3 \
			--min-nodes 1 \
			--max-nodes 20 \
			--machine-type=n1-highmem-2 \
			--enable-autoscaling \
			--enable-autorepair \
			--no-enable-autoupgrade \
			--preemptible \
			--node-labels logstash=yes \
			--node-taints logstash=yes:NoSchedule; \
		echo "ELK node pools created."; \
	else\
		echo "ELK stack is disabled.";\
	fi

# https://cloud.google.com/kubernetes-engine/docs/how-to/gpus#gpu_pool
# When you add a GPU node pool to an existing cluster that already runs a non-GPU
# node pool, GKE automatically taints the GPU nodes with the following node taint
#		--node-taints "nvidia.com/gpu=:NoSchedule"

## Destroy GKE GPU node pool
gke/destroy/node-pools:
	@echo "Destroying node pools."
	@echo "Depending on cluster state, this process could take a while."
	@gke-node-pool-destruction.sh
	@echo "Node pools destroyed."
	@echo " "
	@echo " "

# https://cloud.google.com/storage/docs/access-control/iam-roles
## Create Service Account used by deepcell
gke/create/service-account:
	@echo "Creating GKE service account..."
	@gcloud iam service-accounts create $(CLOUDSDK_CONTAINER_CLUSTER) --display-name "Deepcell" || \
		echo "No need to create service account; it probably already exists."
	@gcloud projects add-iam-policy-binding $(CLOUDSDK_CORE_PROJECT) --member serviceAccount:$(GCP_SERVICE_ACCOUNT) --role roles/storage.admin --no-user-output-enabled
ifneq "" "${CERTIFICATE_MANAGER_ENABLED}"
	@gcloud projects add-iam-policy-binding $(CLOUDSDK_CORE_PROJECT) --member serviceAccount:$(GCP_SERVICE_ACCOUNT) --role roles/dns.admin --no-user-output-enabled
	# @gcloud iam service-accounts add-iam-policy-binding $(GCP_SERVICE_ACCOUNT) \
	# 	--role roles/iam.workloadIdentityUser \
	# 	--member "serviceAccount:$(CLOUDSDK_CORE_PROJECT).svc.id.goog[cert-manager/cert-manager]"
endif
	@echo "GKE service account creation complete."
	@echo " "
	@echo " "

## Delete Service Account used by deepcell
gke/destroy/service-account:
	@echo "Destroying GKE service-account..."
	@gcloud projects remove-iam-policy-binding $(CLOUDSDK_CORE_PROJECT) --member serviceAccount:$(GCP_SERVICE_ACCOUNT) --role roles/storage.admin --no-user-output-enabled
ifneq "" "${CERTIFICATE_MANAGER_ENABLED}"
	@gcloud projects remove-iam-policy-binding $(CLOUDSDK_CORE_PROJECT) --member serviceAccount:$(GCP_SERVICE_ACCOUNT) --role roles/dns.admin --no-user-output-enabled
endif
	@-gcloud iam service-accounts delete $(GCP_SERVICE_ACCOUNT) --quiet
	@echo "GKE service-account destruction finished."
	@echo " "
	@echo " "

## Create Certificate Manager Secret
gke/create/certificate-manager-secret:
ifneq "" "${CERTIFICATE_MANAGER_ENABLED}"
	@gcloud iam service-accounts keys create key.json \
		--iam-account $(GCP_SERVICE_ACCOUNT)
	-@kubectl create namespace cert-manager
	@kubectl -n cert-manager create secret generic clouddns-dns01-solver-svc-acct --from-file=key.json
else
	@echo "Certificate Manager is Disabled"
endif

## Remove Certifiate Manager Secret Key
gke/destroy/certifiate-manager-secret: KEY_ID = $(shell sh -c "cat key.json | jq '.private_key_id' -r" )
gke/destroy/certifiate-manager-secret:
ifneq "" "${CERTIFICATE_MANAGER_ENABLED}"
	@gcloud iam service-accounts keys delete $(KEY_ID) --quiet \
		--iam-account $(GCP_SERVICE_ACCOUNT) || \
		echo "Could not remove key from IAM account."
else
	@echo "Certificate Manager is Disabled"
endif

## Create bucket used by deepcell
gke/create/bucket:
	@echo "Creating Google Cloud Storage Bucket ${CLOUDSDK_BUCKET}..."
	@gsutil mb -p $(CLOUDSDK_CORE_PROJECT) gs://$(CLOUDSDK_BUCKET) \
		|| echo "Bucket ${CLOUDSDK_BUCKET} already exists. No need to create that bucket."
	@-gsutil acl ch -u $(GCP_SERVICE_ACCOUNT):O gs://$(CLOUDSDK_BUCKET)
	@echo "Google Cloud Storage Bucket creation finished."
	@echo " "
	@echo " "

## Install GKE Auth Plugin for kubectl
gke/create/authplugin:
	@echo "Installing the GKE Auth Plugin..."
	@gcloud components install gke-gcloud-auth-plugin --quiet \
		|| echo "Failed to install the GKE Auth Plugin"
	@echo "GKE Auth Plugin install complete."
	@echo " "
	@echo " "

## Destroy bucket used by deepcell
gke/destroy/bucket:
	gsutil rm -r gs://$(CLOUDSDK_BUCKET) || echo "Bucket not destroyed."

## Deploy helm tiller with service account
gke/deploy/helm:
	@echo "Deploying kubernetes resources..."
	@kubectl create serviceaccount --namespace kube-system tiller
	@-kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller
	@-helm init --service-account tiller --upgrade --wait
	@echo "Kubernetes resource deployment finished."
	@echo " "
	@echo " "

gke/destroy/helm:
	-helm reset --force --tiller-connection-timeout=2
	-kubectl delete clusterrolebinding tiller-cluster-rule
	-kubectl delete serviceaccount --namespace kube-system tiller

## Deploy GKE Nvidia drivers
gke/deploy/nvidia:
	@echo "Deploying NVIDIA drivers for GPU instances..."
	@kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded.yaml
	@echo "NVIDIA GPU driver deployment finished."
	@echo " "
	@echo " "

## Create cluster resources, after authentication
gke/create/resources: \
	gke/create/cluster \
	gke/create/certificate-manager-secret \
	gke/create/node-pools \
	gke/create/elk-node-pools \
	gke/create/bucket \
	gke/deploy/nvidia

## Create Cluster
gke/create/all: \
	gke/create/authplugin \
	gke/create/service-account \
	gke/create/resources
	@echo "GKE cluster created"
	@exit 0

## Destroy Cluster
gke/destroy/all: \
	gke/destroy/cluster \
	gke/destroy/certifiate-manager-secret \
	gke/destroy/service-account
	@echo "GKE cluster destroyed"
	@exit 0


# https://cloud.google.com/storage/docs/access-control/iam-roles
# Currently, using Editor and Kubernetes Engine Admin roles for the testing-ci service account.
# It might be possible to replace Editor with someting more specific, but more research would be needed.
## Create Service Account used by deepcell
gke/test/create/service-account:
	@echo "Creating GKE service account..."
	@gcloud auth activate-service-account $(GCP_SERVICE_ACCOUNT) --key-file=$(GOOGLE_APPLICATION_CREDENTIALS)
	@echo "GKE service account creation complete."
	@echo " "
	@echo " "

## Create Cluster
gke/test/create/all: \
	gke/test/create/service-account \
	gke/create/resources
	@echo "GKE cluster created"
	@exit 0

## Destroy Cluster
gke/test/destroy/all: \
	gke/destroy/cluster \
	gke/destroy/certifiate-manager-secret
	@echo "GKE cluster destroyed"
	@exit 0
